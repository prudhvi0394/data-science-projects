{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 384
    },
    "colab_type": "code",
    "id": "zbHOYDi_9n43",
    "outputId": "7144eb0b-7112-48b1-cfd8-584e3c239b2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: catboost in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (0.15)\n",
      "Requirement already satisfied: pandas>=0.19.1 in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from catboost) (1.0.5)\n",
      "Requirement already satisfied: enum34 in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from catboost) (1.1.10)\n",
      "Requirement already satisfied: six in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from catboost) (1.15.0)\n",
      "Requirement already satisfied: graphviz in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from catboost) (0.13.2)\n",
      "Requirement already satisfied: numpy>=1.16.0 in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from catboost) (1.18.1)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\prudhvi\\appdata\\roaming\\python\\python37\\site-packages (from pandas>=0.19.1->catboost) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\prudhvi\\anaconda3\\lib\\site-packages (from pandas>=0.19.1->catboost) (2020.1)\n"
     ]
    }
   ],
   "source": [
    "#Loading some libraries\n",
    "!pip install catboost\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, precision_recall_curve\n",
    "from catboost import Pool, CatBoostClassifier\n",
    "\n",
    "from scipy.stats import pearsonr, chi2_contingency\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.proportion import proportion_confint\n",
    "\n",
    "#for visualization\n",
    "import plotly.express as px\n",
    "\n",
    "%matplotlib inline\n",
    "# figure size in inches\n",
    "rcParams['figure.figsize'] = 14,6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fkCgoBTs9n49"
   },
   "source": [
    "# Loading data into the kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-f5a055d7438b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mPath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"C:\\Users\\Prudhvi\\Downloads\\Github projects\\data-science-projects\\Lending Club\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'Path' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "Path(r\"C:\\Users\\Prudhvi\\Downloads\\Github projects\\data-science-projects\\Lending Club\").cd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "RINo5vNy9n49",
    "outputId": "29044a01-e1e0-48b2-9d6b-764536d51154"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File C:/Users/Prudhvi/loan18-19.csv does not exist: 'C:/Users/Prudhvi/loan18-19.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-d7479596006b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Loading the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"C:/Users/Prudhvi/loan18-19.csv\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdf1\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'issue_d'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfer_datetime_format\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m   \u001b[1;31m#dataset of accepted loans from 2018-2019\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    674\u001b[0m         )\n\u001b[0;32m    675\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 676\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 448\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    449\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    878\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1113\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1114\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1116\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1889\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1891\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1892\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1893\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] File C:/Users/Prudhvi/loan18-19.csv does not exist: 'C:/Users/Prudhvi/loan18-19.csv'"
     ]
    }
   ],
   "source": [
    "#Loading the data\n",
    "file = \"C:/Users/Prudhvi/loan18-19.csv\"\n",
    "df1=pd.read_csv(file,parse_dates=['issue_d'], infer_datetime_format=True)   #dataset of accepted loans from 2018-2019\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "j43S5WKi9n5D",
    "outputId": "5a9a766d-8471-417a-8dd2-34ae81fe2186"
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dqk6W7xh9n5H"
   },
   "source": [
    "The size of the dataframe is too large for analysis so it's necessary to reduce the amount of datapoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "edkl9q3s9n5L"
   },
   "outputs": [],
   "source": [
    "\n",
    "df1.head()\n",
    "df1.reset_index(drop=True)        #resetting the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U7B1JR2z9n5P"
   },
   "outputs": [],
   "source": [
    "df1.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Zzdw2s69n5T"
   },
   "source": [
    "## Getting dictionary of names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZxABGxXG9n5V"
   },
   "outputs": [],
   "source": [
    "inv_view = pd.read_excel(\"C:/Users/Prudhvi/LCD.xlsx\",\n",
    "                             sheet_name=1)\n",
    "inv_view.head(20)\n",
    "inv_view.info()\n",
    "inv_feat = inv_view['BrowseNotesFile'].dropna().values\n",
    "inv_feat = [re.sub('(?<![0-9_])(?=[A-Z0-9])', '_', x).lower().strip() for x in inv_feat]  #the column value names are different from the original dataset since this is a view shown to investors only whereas original dataset has more columns \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AyRjPmTH9n5c"
   },
   "source": [
    "## Lending kart data has columns of data which are relevant only after loan is issued so removing those which aren't available at the time of investing\n",
    "\n",
    "Lending Kart helps connect borrowers with investors.There are certain variables in data which are available once the loan has been disbursed.\n",
    "\n",
    "We are going to take only variables which are useful before loan has been given to borrowers.This will help us to predict factors which are responsible for good loans.The way it generally works is that borrowers with high credit score get lower interest rates.\n",
    "\n",
    "But investors will get less returns with lower interest rates so they are interested in loans with higher interest rates, which are classified as risky due to which they are having higher interest rates.So it would be a useful proposition for the business to classify loans accurately aomng high interest loans.\n",
    "\n",
    "To get a list of such variables we can see their dictionary sheet in which they have given investor columns view.Here we are trying to predict good loans amoung high risk/high interest rate loans. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jTTjC_X-9n5d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#checking for different columns in both sets, the view to investor and view of the company\n",
    "feat=df1.columns.values\n",
    "np.setdiff1d(inv_feat,feat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uG6MWdSR9n5i"
   },
   "outputs": [],
   "source": [
    "np.setdiff1d(feat,inv_feat)   #checking for values in array 1 which aren't in array 2\n",
    "\n",
    "# we see that some columns are there in both but spellings are different so changing the spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "321dFagK9n5l"
   },
   "outputs": [],
   "source": [
    "wrong = ['is_inc_v', 'mths_since_most_recent_inq', 'mths_since_oldest_il_open',\n",
    "         'mths_since_recent_loan_delinq', 'verified_status_joint']\n",
    "correct = ['verification_status', 'mths_since_recent_inq', 'mo_sin_old_il_acct',\n",
    "           'mths_since_recent_bc_dlq', 'verification_status_joint']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xGREZvxY9n5p"
   },
   "outputs": [],
   "source": [
    "inv_feat = np.setdiff1d(inv_feat, wrong)   #subtracting wrong columns from the dataset \n",
    "inv_feat = np.append(inv_feat, correct)    #appending correct values below\n",
    "inv_feat1 = np.intersect1d(inv_feat, feat)\n",
    "df2= df1[inv_feat1].copy()\n",
    "df2.info()  #intersection of columns values available for the investors\n",
    "df2.select_dtypes('object').head()  #checking if there are any datatypes assigned wrongly as strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iIB9ycnUTDwu"
   },
   "outputs": [],
   "source": [
    "df2.select_dtypes('object').head()  #checking if there are any datatypes assigned wrongly as strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gGeCnCSd9n50"
   },
   "source": [
    "1)Need to change 'earliest_cr_line' & 'sec_app_earliest_cr_line' from strings to dates\n",
    "\n",
    "2)emp_length and id are numeric and their type should be changed to float. In case of emp_length I replace the extreme cases of \"< 1 year\" and \"10+ years\" with \"0 years\" and \"11 years\" respectively to separate these groups from the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEydDf7J9n51"
   },
   "outputs": [],
   "source": [
    "df2['earliest_cr_line'] = pd.to_datetime(df2['earliest_cr_line'], infer_datetime_format=True)\n",
    "df2['sec_app_earliest_cr_line'] = pd.to_datetime(df2['sec_app_earliest_cr_line'], infer_datetime_format=True)\n",
    "\n",
    "df2['emp_length'] = df2['emp_length'].replace({'< 1 year': '0 years', '10+ years': '11 years'})\n",
    "df2['emp_length'] = df2['emp_length'].str.extract('(\\d+)').astype('float')\n",
    "df2['id'] = df2['id'].astype('float')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7JoiwKz69n54"
   },
   "source": [
    "## EDA of loan amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gadA9KEI9n54"
   },
   "outputs": [],
   "source": [
    "df1['int_round'] = df1['int_rate'].round(0).astype(int)  #rounding the interest rates\n",
    "df1.info()\n",
    "#creating a figure to plot on\n",
    "\n",
    "loan_amount = df1[\"loan_amnt\"].values\n",
    "funded_amount = df1[\"funded_amnt\"].values\n",
    "investor_funds = df1[\"funded_amnt_inv\"].values\n",
    "\n",
    "plt.figure(figsize=(20,14))\n",
    "\n",
    "#plotting distribution of loan amount\n",
    "plt.subplot(211)\n",
    "g = sns.distplot(loan_amount)\n",
    "g.set_xlabel(\"Loan Amount Value\", fontsize=16)\n",
    "g.set_ylabel(\"Frequency\", fontsize=16)\n",
    "g.set_title(\"Loan Amount Distribuition\", fontsize=20)\n",
    "\n",
    "#plotting the overall fulfillment rate by the company of the loans requested\n",
    "plt.figure(figsize=(20,14))\n",
    "\n",
    "plt.subplot(212)\n",
    "f = sns.distplot(funded_amount)\n",
    "f.set_xlabel(\"Funded amount value\", fontsize=16)\n",
    "f.set_ylabel(\"Frequency\", fontsize=16)\n",
    "f.set_title(\"Funded Amount Distribution\", fontsize=20)\n",
    "\n",
    "#plotting the distribution of loan amount fulfillment by investors\n",
    "plt.figure(figsize=(20,14))\n",
    "\n",
    "plt.subplot(212)\n",
    "h = sns.distplot(investor_funds)\n",
    "h.set_xlabel(\"Investor funded amount value\", fontsize=16)\n",
    "h.set_ylabel(\"Frequency\", fontsize=16)\n",
    "h.set_title(\"Investor Funded Amount Distribution\", fontsize=20)\n",
    "\n",
    "df1['int_round'].describe()\n",
    "total = len(df1)\n",
    "plt.figure(figsize=(20,16))\n",
    "plt.subplot(212)\n",
    "g1 = sns.countplot(x=\"int_round\", data=df1, \n",
    "                  color='blue')\n",
    "g1.set_xlabel(\"Loan Interest Rate\", fontsize=16)\n",
    "g1.set_ylabel(\"Count\", fontsize=16)\n",
    "g1.set_title(\"Interest Rate Distribuition\", fontsize=20)\n",
    "sizes=[] # Get highest values in y\n",
    "for p in g1.patches:\n",
    "    height = p.get_height()\n",
    "    sizes.append(height)\n",
    "    g1.text(p.get_x()+p.get_width()/2.,\n",
    "            height + 3,\n",
    "            '{:1.2f}%'.format(height/total*100),\n",
    "            ha=\"center\", fontsize=12) \n",
    "g1.set_ylim(0, max(sizes) * 1.10) # set y limit based on highest heights\n",
    "\n",
    "plt.subplots_adjust(hspace = 0.4,top = 0.9)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YcL_Xr2X9n6H"
   },
   "source": [
    "## Checking for missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ieGEWBgn9n6I"
   },
   "source": [
    "Once we are done with a basic analysis of the numerical variables like loan,interest etc we are going to check if there are any missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2dIsUjL39n6I"
   },
   "outputs": [],
   "source": [
    "nan_mean = df2.isna().mean()\n",
    "nan_mean = nan_mean[nan_mean != 0].sort_values()\n",
    "nan_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WV5sez0g9n6K"
   },
   "outputs": [],
   "source": [
    "df2 = df2.drop(['desc', 'member_id'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjGGYX6K9n6N"
   },
   "source": [
    "Removing these two columns as they are completely empty so let's remove them as there are only missing values in them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u2fCweSD9n6O"
   },
   "source": [
    "## Handling Missing values\n",
    "Taking care of missing values in the above columns by filling them appropriately.\n",
    "There are some categorical variables which can be filled with empty strings.To check which ones are categorical variables we can check in the cleaned dataset and take a difference of sets twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wet-JYQpUz8Q"
   },
   "outputs": [],
   "source": [
    "f=list(nan_mean.index)\n",
    "f\n",
    "cat_cols=df2.select_dtypes('object').columns.values\n",
    "cat_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RtuG-i0k9n6O",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding out which are categorical variables from f\n",
    "set1=np.setdiff1d(f,cat_cols)  #getting list of numerical variables from f by subtracting the categorical columns from cleaned df2\n",
    "set2=np.setdiff1d(f,set1)     #subtracting the list of numerical from f to get back the categorical columns left in this f\n",
    "set2                         #using these to set the empty string to these categorical columns missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SRy8uLX69n6T"
   },
   "source": [
    "We find that there are two categorical variables,so their missing values can be filled by an empty string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ghMS4gRr9n6T"
   },
   "outputs": [],
   "source": [
    "fill_empty = ['emp_title', 'verification_status_joint']\n",
    "fill_max = ['bc_open_to_buy', 'mo_sin_old_il_acct', 'mths_since_last_delinq',\n",
    "            'mths_since_last_major_derog', 'mths_since_last_record',\n",
    "            'mths_since_rcnt_il', 'mths_since_recent_bc', 'mths_since_recent_bc_dlq',\n",
    "            'mths_since_recent_inq', 'mths_since_recent_revol_delinq',\n",
    "            'pct_tl_nvr_dlq','sec_app_mths_since_last_major_derog']\n",
    "fill_min = np.setdiff1d(df2.columns.values, np.append(fill_empty, fill_max))\n",
    "\n",
    "\n",
    "df2[fill_empty] = df2[fill_empty].fillna('')\n",
    "df2[fill_max] = df2[fill_max].fillna(df2[fill_max].max())\n",
    "df2[fill_min] = df2[fill_min].fillna(df2[fill_min].min()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NG8q-fdp9n6Y"
   },
   "outputs": [],
   "source": [
    "#checking again for missing values\n",
    "nan_mean = df2.isna().mean()\n",
    "nan_mean = nan_mean[nan_mean != 0].sort_values()\n",
    "nan_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bjbwVzpD9n6b"
   },
   "source": [
    "There are some numerical variables which have to be filled with maximum of their respective columns and there are some which have to be filled with min of their column.\n",
    "\n",
    "For example the 'feature mths_since_last_record' indicates the number of months since the last record (like bankruptcy, foreclosure, tax liens, etc.) so if missing, one should assume that no records were made and the number of months since the \"last\" record should be a maximum.\n",
    "\n",
    " the feature emp_length indicates the number of working years so if missing, one should assume that the borrower never worked and the number of working years should be a minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIt6-mew9n6b"
   },
   "source": [
    "## Checking for multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BL-co9ZO9n6c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting numerical values for multicollinearity\n",
    "numerical = df2.select_dtypes('number').columns.values\n",
    "df2[numerical].nunique().sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iJhgNr2E9n6e"
   },
   "source": [
    " We can see that number id has 0 values and id has amount of values equivalent to number of rows so we have to remove these two as they are redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zI08QukY9n6e"
   },
   "outputs": [],
   "source": [
    "df2 = df2.drop(['num_tl_120dpd_2m', 'id'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JxOGOK-f9n6h"
   },
   "outputs": [],
   "source": [
    "#calculating collinearity of all the remaining pairs \n",
    "num_feat = df2.select_dtypes('number').columns.values\n",
    "comb_num_feat = np.array(list(combinations(num_feat, 2)))\n",
    "corr_num_feat = np.array([])\n",
    "for comb in comb_num_feat:\n",
    "   corr = pearsonr(df2[comb[0]], df2[comb[1]])[0]\n",
    "   corr_num_feat = np.append(corr_num_feat, corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XAJ81WhD9n6k"
   },
   "outputs": [],
   "source": [
    "#checking for missing values in correlation matrix\n",
    "corr_num_feat[np.isnan(corr_num_feat)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wlPSJR4e9n6n"
   },
   "outputs": [],
   "source": [
    "# We will check for values with collinearity above 0.9\n",
    "high_corr_num = comb_num_feat[np.abs(corr_num_feat) >= 0.9]\n",
    "high_corr_num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lxrwOfeb9n6s"
   },
   "outputs": [],
   "source": [
    "# We have to remove either of the two values from each list in the array,so removing the first element of every list\n",
    "\n",
    "df2= df2.drop(np.unique(high_corr_num[:, 0]), axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DJrFxuQD9n6u"
   },
   "outputs": [],
   "source": [
    "#Getting categorical variables to calculate correlation\n",
    "\n",
    "cat_feat = df2.select_dtypes('object').columns.values\n",
    "df2[cat_feat].nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KY_oaV6W9n6w"
   },
   "outputs": [],
   "source": [
    "df2 = df2.drop(['url', 'emp_title'], axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AYDjBH49n6z"
   },
   "source": [
    "Url has as many unique values as the number of rows and emp_title also has a lot of unique values to calculate correlation so removing both of them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RyL5YZYe9n6z"
   },
   "source": [
    "Calculating Cramer's V correlation for categorical variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSD-vw8g9n60"
   },
   "outputs": [],
   "source": [
    "np.seterr(divide='ignore', invalid='ignore')\n",
    "cat_feat = df2.select_dtypes('object').columns.values\n",
    "comb_cat_feat = np.array(list(combinations(cat_feat, 2)))\n",
    "corr_cat_feat = np.array([])\n",
    "for comb in comb_cat_feat:\n",
    "    table = pd.pivot_table(df2, values='loan_amnt', index=comb[0], columns=comb[1], aggfunc='count').fillna(0)\n",
    "    corr = np.sqrt(chi2_contingency(table)[0] / (table.values.sum() * (np.min(table.shape) - 1) ) )\n",
    "    corr_cat_feat = np.append(corr_cat_feat, corr)\n",
    "corr_cat_feat    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-xJ99ob29n62"
   },
   "outputs": [],
   "source": [
    "high_corr_cat = comb_cat_feat[corr_cat_feat >= 0.9]\n",
    "high_corr_cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N0XTyIMp9n65"
   },
   "outputs": [],
   "source": [
    "#Removing one of the two variables from each list object\n",
    "\n",
    "df2 = df2.drop(np.unique(high_corr_cat[:, 1]), axis=1, errors='ignore')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LlYOfIDU9n68"
   },
   "source": [
    "## Checking the status of loans in the main data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XU1rQdHl9n69"
   },
   "outputs": [],
   "source": [
    "#As we want to classify good loans and bad loans.\n",
    "# We need to first specify what consists of a good loan and bad loan respectively with the help of loan statuses.\n",
    "\n",
    "status=pd.DataFrame(df1['loan_status'].value_counts())\n",
    "status=status.T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TnREqzZK9n7A"
   },
   "outputs": [],
   "source": [
    "#Assigning good and bad loans based on following assumptions\n",
    "#1)Current and Fully paid loans are definintely good loans,in Grace period can be considered a good or bad loan\n",
    "# let's consider it as good for this purpose,rest are all classified as bad\n",
    "\n",
    "x = df1['loan_status'].copy()\n",
    "x = x.isin(['Current', 'Fully Paid', 'In Grace Period']).astype('int')\n",
    "x.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "snT3IPnX9n7E"
   },
   "source": [
    "## Next step after studying the data and classifying is modelling\n",
    "\n",
    "The next step is to identify high risk loans based on the parameters in the data since as we had discussed we want to predict loans amoung high risk loans.\n",
    "\n",
    "There are two features interest rate and the grade which assigns risk to loans.So either one of the features can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PQR-lsVp9n7K"
   },
   "outputs": [],
   "source": [
    "grades = pd.DataFrame(df2['grade'].value_counts()).transpose()\n",
    "grades.iloc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdjzLOzf9n7N"
   },
   "outputs": [],
   "source": [
    "plt.bar(status.columns, status.iloc[0], align='center', alpha=0.5)\n",
    "\n",
    "plt.xlabel('Loan grades')\n",
    "plt.title('Distribution of loan grades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dzr1YXa09n7R"
   },
   "outputs": [],
   "source": [
    "# Getting the grade E loans data as we are predicting good loans out of high risk loans \n",
    "# We will be using only these loans for creating the model\n",
    "df_mod = df2[df2.grade == 'E'].copy()\n",
    "df_mod.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0urAtbm39n7U"
   },
   "outputs": [],
   "source": [
    "df_mod = df_mod.drop([ 'grade','int_rate'], axis=1, errors='ignore')\n",
    "x_mod = x[df_mod.index]                  #x consists of the column of target values classified on basis of loan status,filtering only grade E loans in the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj4h_vpc9n7X"
   },
   "outputs": [],
   "source": [
    "#splitting into training,validation and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_mod, x_mod, stratify=x_mod, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s6aURlu99n7b"
   },
   "outputs": [],
   "source": [
    "cat_feat_ind = (X_train.dtypes == 'object').to_numpy().nonzero()[0]\n",
    "pool_train = Pool(X_train, y_train, cat_features=cat_feat_ind)\n",
    "pool_val = Pool(X_val, y_val, cat_features=cat_feat_ind)\n",
    "pool_test = Pool(X_test, y_test, cat_features=cat_feat_ind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MSOI6cj19n7d"
   },
   "outputs": [],
   "source": [
    "n = y_train.value_counts()\n",
    "model = CatBoostClassifier(learning_rate=0.03,\n",
    "                           iterations=1000,\n",
    "                           early_stopping_rounds=100,\n",
    "                           class_weights=[1, n[0] / n[1]],\n",
    "                           verbose=False,\n",
    "                           random_state=0)\n",
    "model.fit(pool_train, eval_set=pool_val, plot=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "naMdTXvXtxc-"
   },
   "source": [
    "Since the validation dataset was used to tune hyperparameters (the number of iterations), I predict targets for the testing dataset which the model hasn't seen yet. The metrics reported here are accuracy, precision and recall. They all have sensible values which is also confirmed by the confusion matrix shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ygsIxmnm0EN"
   },
   "outputs": [],
   "source": [
    "y_pred_test = model.predict(pool_test)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_pred_test)\n",
    "prec_test = precision_score(y_test, y_pred_test)\n",
    "rec_test = recall_score(y_test, y_pred_test)\n",
    "print(f'''Accuracy (test): {acc_test:.3f}\n",
    "Precision (test): {prec_test:.3f}\n",
    "Recall (test): {rec_test:.3f}''')\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "ax = sns.heatmap(cm, cmap='viridis_r', annot=True, fmt='d', square=True)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vIp1CnEN3ED0"
   },
   "source": [
    "# Feature importance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TggP-eCnDFZ"
   },
   "outputs": [],
   "source": [
    "feat = model.feature_names_\n",
    "imp = model.feature_importances_\n",
    "df = pd.DataFrame({'Feature': feat, 'Importance': imp})\n",
    "df = df.sort_values('Importance', ascending=False)[:10]\n",
    "sns.barplot(x='Importance', y='Feature', data=df);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YkxQim_E07Hw"
   },
   "outputs": [],
   "source": [
    "good = df_mod.loc[x_mod == 1, 'loan_amnt']\n",
    "bad = df_mod.loc[x_mod == 0, 'loan_amnt']\n",
    "\n",
    "bins = 20\n",
    "sns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\n",
    "ax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4-jfS6ac1vSI"
   },
   "outputs": [],
   "source": [
    "good = df_mod.loc[x_mod == 1, 'mths_since_recent_inq']\n",
    "bad = df_mod.loc[x_mod == 0, 'mths_since_recent_inq']\n",
    "\n",
    "bins = 20\n",
    "sns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\n",
    "ax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cc7bJbrl2L8i"
   },
   "outputs": [],
   "source": [
    "good = df_mod.loc[x_mod == 1, 'revol_util']\n",
    "bad = df_mod.loc[x_mod == 0, 'revol_util']\n",
    "\n",
    "bins = 20\n",
    "sns.distplot(good, bins=bins, label='Good loans', kde=False, norm_hist=True)\n",
    "ax = sns.distplot(bad, bins=bins, label='Bad loans', kde=False, norm_hist=True)\n",
    "ax.set_ylabel('Density')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYgTpx_p2wXW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O9gPrD8I3Kpw"
   },
   "source": [
    "# Model adjustment to increase precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "viZhQKAp3SrT"
   },
   "outputs": [],
   "source": [
    "y_proba_val = model.predict_proba(pool_val)[:, 1]\n",
    "p_val, r_val, t_val = precision_recall_curve(y_val, y_proba_val)\n",
    "plt.plot(r_val, p_val)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wH3BUJnF7vNO"
   },
   "outputs": [],
   "source": [
    "p_max = p_val[p_val != 1].max()\n",
    "t_all = np.insert(t_val, 0, 0)\n",
    "t_adj_val = t_all[p_val == p_max]\n",
    "y_adj_val = (y_proba_val > t_adj_val).astype(int)\n",
    "p_adj_val = precision_score(y_val, y_adj_val)\n",
    "print(f'Adjusted precision (validation): {p_adj_val:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctl2giGN8oyd"
   },
   "outputs": [],
   "source": [
    "n = y_adj_val.sum()\n",
    "ci = proportion_confint(p_adj_val * n, n, alpha=0.05, method='wilson')\n",
    "print(f'95% confidence interval for adjusted precision: [{ci[0]:.3f}, {ci[1]:.3f}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wri_z_2h8-qx"
   },
   "outputs": [],
   "source": [
    "y_proba_test = model.predict_proba(pool_test)[:, 1]\n",
    "y_adj_test = (y_proba_test > t_adj_val).astype(int)\n",
    "p_adj_test = precision_score(y_test, y_adj_test)\n",
    "r_adj_test = recall_score(y_test, y_adj_test)\n",
    "print(f'''Adjusted precision (test): {p_adj_test:.3f}\n",
    "Adjusted recall (test): {r_adj_test:.3f}''')\n",
    "\n",
    "cm_test = confusion_matrix(y_test, y_adj_test)\n",
    "ax = sns.heatmap(cm_test, cmap='viridis_r', annot=True, fmt='d', square=True)\n",
    "ax.set_xlabel('Predicted')\n",
    "ax.set_ylabel('True');"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Lendingclub2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
